{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbVwGI9oXuvH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.28.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import spacy\n",
        "import spacy.cli\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# for spaCy Text Categorizer\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training.example import Example\n",
        "\n",
        "from wordcloud import WordCloud # See : https://www.kaggle.com/aashita/word-clouds-of-various-shapes\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F # for softmax in Eval\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz # timezone\n",
        "\n",
        "from tqdm import trange # used to make our loops show a smart progress meter\n",
        "\n",
        "# from autocorrect import spell # (TODO: do we really need this?)\n",
        "\n",
        "#torch.manual_seed(0)\n",
        "\n",
        "print(\"Using PyTorch version\", torch.__version__)"
      ],
      "metadata": {
        "id": "NqsJ_w4tXzQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Cheating Detection for AI generated/df_score_predictor.csv')\n"
      ],
      "metadata": {
        "id": "8g8hFWR7YFl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 0, 1, 2 with 1 in the 'label' column\n",
        "'''\n",
        "df['final_AP_score'] = df['final_AP_score'].replace([0, 1, 2], 0)\n",
        "df['final_AP_score'] = df['final_AP_score'].replace([3], 1)\n",
        "df['final_AP_score'] = df['final_AP_score'].replace([4], 2)\n",
        "df['final_AP_score'] = df['final_AP_score'].replace([5], 3)\n",
        "df['final_AP_score'] = df['final_AP_score'].replace([6], 4)\n",
        "'''"
      ],
      "metadata": {
        "id": "L3xhb3W87Z5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_auto_eda = False # run sweetviz report\n",
        "is_small_spacy = True\n",
        "is_sample_df = True\n",
        "sample_fraction = 0.1"
      ],
      "metadata": {
        "id": "6e6yxMR0YTs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\") # (TODO: change \"cpu\" to \"device error\"?)\n",
        "device"
      ],
      "metadata": {
        "id": "AOm7IlnYYF2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re,string,unicodedata\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet, stopwords"
      ],
      "metadata": {
        "id": "o4GW_Q02YWGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if is_small_spacy:\n",
        "  spacy.cli.download(\"en_core_web_sm\")\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "else:\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "  nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "0pNWYtLyYX6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(df):\n",
        "    pattern = r\"<p>----Response for Part A----</p><p>(.*?)</p>\"\n",
        "    df['extracted_text'] = df['response'].str.extract(pattern, flags=re.DOTALL)\n",
        "    return df"
      ],
      "metadata": {
        "id": "N1DhPSn61qzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_text(df)\n",
        "df"
      ],
      "metadata": {
        "id": "bV-QqRJz2gwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['extracted_text'].isna().sum()"
      ],
      "metadata": {
        "id": "yCX7dKbM97np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['extract_text'] = df['extracted_text'].fillna('No Response form the Student')"
      ],
      "metadata": {
        "id": "MWMbcQZi-Cfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['preprocessed_text'] = list(nlp.pipe(df['extract_text']))"
      ],
      "metadata": {
        "id": "Y7olS-ek-0mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_doc(doc):\n",
        "  list_clean_tokens =  [token for token in doc if\n",
        "              not token.is_punct # punctuation\n",
        "              and not token.is_currency\n",
        "              and not token.is_digit\n",
        "              # and not token.is_oov # Is the token out-of-vocabulary (i.e. does it not have a word vector)?\n",
        "              and not token.is_space # Does the token consist of whitespace characters? Equivalent to token.text.isspace().\n",
        "              and not token.is_stop\n",
        "              and not token.like_num\n",
        "              and not token.like_url and ('@' not in token.text) and ('|' not in token.text)\n",
        "              # and not token.pos_ == \"PROPN\" (\"Wikipedia\")\n",
        "              ]\n",
        "  return list_clean_tokens\n",
        "\n",
        "# Lemmatizing + to lower case\n",
        "def lemma_text(doc):\n",
        "    tokens=[]\n",
        "    for token in doc:\n",
        "        if token.lemma_ != \"-PRON-\":\n",
        "            lemma = token.lemma_.lower().strip()\n",
        "        else:\n",
        "            lemma = token.lower_\n",
        "        tokens.append(lemma)\n",
        "    return tokens\n",
        "\n",
        "#create_string\n",
        "def create_string(doc):\n",
        "  new_string = ' '.join([str(token) for token in doc])\n",
        "  return new_string"
      ],
      "metadata": {
        "id": "qj4wTNGYgEu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_all(df):\n",
        "    df['preprocessed_text'] = df['preprocessed_text'].apply(clean_doc)\n",
        "    df['preprocessed_text'] = df['preprocessed_text'].apply(lemma_text)\n",
        "    df['preprocessed_text'] = df['preprocessed_text'].apply(create_string)\n",
        "    return df"
      ],
      "metadata": {
        "id": "r3OPtB97gU2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessed = preprocess_all(df)"
      ],
      "metadata": {
        "id": "yq6oz484gbNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessed"
      ],
      "metadata": {
        "id": "8DLhwpCwgcqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import (ElectraForSequenceClassification,RobertaTokenizer, RobertaForSequenceClassification,\n",
        "                          AutoTokenizer, EvalPrediction, InputFeatures,AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          Trainer, TrainingArguments, glue_compute_metrics)\n"
      ],
      "metadata": {
        "id": "aHUTrRMLaTnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "IE0gIjjw6qKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "metadata": {
        "id": "-lcZhZ_I9yZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade safetensors"
      ],
      "metadata": {
        "id": "XYvVVWl7_to2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, XLMRobertaXLForSequenceClassification, XLMRobertaForSequenceClassification"
      ],
      "metadata": {
        "id": "gLpQNTbn-jjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['final_AP_score'].value_counts()"
      ],
      "metadata": {
        "id": "B_-86PMa72aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-large',\n",
        "                                                           num_labels=6,\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           ignore_mismatched_sizes=True\n",
        "                                                           )"
      ],
      "metadata": {
        "id": "EGxRMe79OgwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\", num_labels=6, ignore_mismatched_sizes=True)\n"
      ],
      "metadata": {
        "id": "-h0Lk8B4aT1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainerDataset(Dataset):\n",
        "    def __init__(self, inputs, targets, tokenizer):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Tokenize the input\n",
        "        self.tokenized_inputs = tokenizer(inputs, padding=True, max_length=256, add_special_tokens=True, truncation=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return InputFeatures(\n",
        "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
        "            token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n",
        "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
        "            label=self.targets[idx])"
      ],
      "metadata": {
        "id": "T8Isziy3adUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid = train_test_split(df, test_size=0.2, stratify = df['final_AP_score'])"
      ],
      "metadata": {
        "id": "1hAlax8SbPZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['preprocessed_text'].isna().sum()"
      ],
      "metadata": {
        "id": "g3yAjOKbbvmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "Ihj3aR9vb5wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "y_train = X_train[\"final_AP_score\"].values\n",
        "# Assuming y_train is a 1D array or a pandas Series\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_valid = X_valid[\"final_AP_score\"].values\n",
        "y_valid = y_valid.reshape(-1, 1)\n",
        "categories = [[0, 1, 2, 3, 4, 5]]\n",
        "encoder = OneHotEncoder(categories = categories, sparse=False)\n",
        "\n",
        "# Fit and transform y_train to one-hot encoded format\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_valid_encoded = encoder.fit_transform(y_valid)\n",
        "\n"
      ],
      "metadata": {
        "id": "a0ZD4ZAjR1oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_encoded"
      ],
      "metadata": {
        "id": "rNCuUyWfSciE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrainerDataset(X_train[\"response\"].tolist(),\n",
        "                               X_train[\"final_AP_score\"].tolist(), tokenizer)\n",
        "eval_dataset = TrainerDataset(X_valid[\"response\"].tolist(),\n",
        "                               X_valid[\"final_AP_score\"].tolist(), tokenizer)"
      ],
      "metadata": {
        "id": "KAsvtcbiaeFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "p1LmCQF6LVRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    y_true = pred.label_ids\n",
        "    y_pred = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'qwk': qwk\n",
        "    }\n"
      ],
      "metadata": {
        "id": "CLkNowBPeijP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='.',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "sHavu5tlQx0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./models/model_electra2_prompt_4\",\n",
        "  num_train_epochs=4,  # 1 (1 epoch gives slightly lower accuracy)\n",
        "  overwrite_output_dir=True,\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  learning_rate=2e-5  ,\n",
        "  lr_scheduler_type  = 'linear',\n",
        "  adam_beta1 = 0.9,\n",
        "  adam_beta2 = 0.999,\n",
        "  adam_epsilon = 5e-06,\n",
        "  weight_decay = 0.01,\n",
        "  per_device_train_batch_size=8,\n",
        "  per_device_eval_batch_size=8,\n",
        "  save_total_limit = 2,\n",
        "  save_strategy = 'epoch',\n",
        "  load_best_model_at_end=False\n",
        "                                )\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the Trainer class\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "xwibHTVbbryA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GIg12UhReq4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.read_csv('/content/drive/MyDrive/Cheating Detection for AI generated/Results/result.csv')"
      ],
      "metadata": {
        "id": "UQDrxZkDyyIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print and store evaluation results\n",
        "def print_and_store_results(y_test, y_pred, model_name, results_df):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_test, y_pred, weights='quadratic')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"Quadratic Kappa: {kappa:.4f}\")\n",
        "    target_names = ['1','2', '3', '4', '5']\n",
        "\n",
        "    results_df = results_df.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'Quadratic Kappa': kappa,\n",
        "    }, ignore_index=True)\n",
        "\n",
        "    # Plot confusion matrix as heatmap\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(model_name)\n",
        "    plt.savefig(f'/content/drive/MyDrive/Cheating Detection for AI generated/Results/{model_name}_confusion_matrix.png')\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "THkvn1gWyoxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = trainer.predict(test_dataset=eval_dataset).predictions"
      ],
      "metadata": {
        "id": "D6yuu-Y_U0la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = trainer.predict(test_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "3dbMx8KzW0wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.argmax(y_actual.label_ids, axis=1)"
      ],
      "metadata": {
        "id": "JHJ8WhOKW8Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labesl = np.argmax(probs, axis=1)"
      ],
      "metadata": {
        "id": "fDngMpulWe39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = print_and_store_results(y_true,pred_labesl, 'roberta_large',results_df )"
      ],
      "metadata": {
        "id": "Z7C1EEeRU3nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred, actual, _ = trainer.predict(eval_dataset)\n",
        "pred_labels = pred.argmax(-1)\n",
        "\n",
        "results_df = print_and_store_results(actual,pred_labels, 'bert_large',results_df )"
      ],
      "metadata": {
        "id": "lwtm_dTDerFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "r2ox1kEVZC5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('/content/drive/MyDrive/Cheating Detection for AI generated/Results/result.csv', index = False)"
      ],
      "metadata": {
        "id": "4DmrJhnxkrAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_90gP_glvxX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}